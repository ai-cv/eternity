{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import jieba\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = '../data/pos/'\n",
    "neg_path = '../data/neg/'\n",
    "pos_list = os.listdir(pos_path)\n",
    "neg_list = os.listdir(neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词词典\n",
    "stop_words = []\n",
    "# 读取停用词函数\n",
    "with open(\"../data/stop_words_line.txt\", encoding=\"utf-8\") as st:\n",
    "    stop_words = st.readline().split(\" \")\n",
    "\n",
    "def delete_stop_words(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def str_strim(line):\n",
    "    new_line = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", line)\n",
    "    return new_line.strip().strip().replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "def segment(line):\n",
    "    return delete_stop_words(jieba.cut(str_strim(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.687 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 词语索引\n",
    "index = 1\n",
    "# 所有词的索引词典\n",
    "words_dict = {}\n",
    "\n",
    "origin_data = []\n",
    "train_label = []\n",
    "for doc in pos_list:\n",
    "    with open(pos_path + doc, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = segment(line)\n",
    "            origin_data.append(words)\n",
    "            train_label.append(1)\n",
    "            for word in words:\n",
    "                if word not in words_dict:\n",
    "                    words_dict[word] = index\n",
    "                    index += 1\n",
    "            \n",
    "for doc in neg_list:\n",
    "    with open(neg_path + doc, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = segment(line)\n",
    "            origin_data.append(words)\n",
    "            train_label.append(0)\n",
    "            for word in words:\n",
    "                if word not in words_dict:\n",
    "                    words_dict[word] = index\n",
    "                    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1754, ('Choice', 67))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index, words_dict.popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 487)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(origin_data), len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讲原始数据中文转化为索引\n",
    "# 将数组转换为表示单词出现与否的由 0 和 1 组成的向量，类似于 one-hot 编码。例如，序列[3, 5]将转换为一个 10,000 维的向量，该向量除了索引为 3 和 5 的位置是 1 以外，其他都为 0。然后，将其作为网络的首层——一个可以处理浮点型向量数据的稠密层。不过，这种方法需要大量的内存，需要一个大小为 num_words * num_reviews 的矩阵。\n",
    "# 或者，我们可以填充数组来保证输入数据具有相同的长度，然后创建一个大小为 max_length * num_reviews 的整型张量。我们可以使用能够处理此形状数据的嵌入层作为网络中的第一层。\n",
    "train_data = []\n",
    "for words in origin_data:\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        if word not in words_dict:\n",
    "            continue\n",
    "        vector.append(int(words_dict.get(word)))\n",
    "#     train_data.append(np.array(vector))\n",
    "    train_data.append(vector)\n",
    "\n",
    "# train_data = np.array(train_data)\n",
    "# train_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, list, list, int)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), type(train_data), type(train_data[0]), type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data = keras.preprocessing.sequence.pad_sequences([[1,2],[4,6,7,8]],padding='post',maxlen=1024)\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,padding='post',maxlen=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  0  0 ...  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 层按顺序堆叠以构建分类器：\n",
    "# 第一层是嵌入（Embedding）层。该层采用整数编码的词汇表，并查找每个词索引的嵌入向量（embedding vector）。这些向量是通过模型训练学习到的。向量向输出数组增加了一个维度。得到的维度为：(batch, sequence, embedding)。\n",
    "# 接下来，GlobalAveragePooling1D 将通过对序列维度求平均值来为每个样本返回一个定长输出向量。这允许模型以尽可能最简单的方式处理变长输入。\n",
    "# 该定长输出向量通过一个有 16 个隐层单元的全连接（Dense）层传输。\n",
    "# 最后一层与单个输出结点密集连接。使用 Sigmoid 激活函数，其函数值为介于 0 与 1 之间的浮点数，表示概率或置信度。\n",
    "\n",
    "# 输入形状是用于电影评论的词汇数目（10,000 词）\n",
    "vocab_size = 2048\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          32768     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 33,057\n",
      "Trainable params: 33,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "# 一个模型需要损失函数和优化器来进行训练。由于这是一个二分类问题且模型输出概率值（一个使用 sigmoid 激活函数的单一单元层），\n",
    "# 我们将使用 binary_crossentropy 损失函数。这不是损失函数的唯一选择，例如，您可以选择 mean_squared_error 。\n",
    "# 但是，一般来说 binary_crossentropy 更适合处理概率——它能够度量概率分布之间的“距离”，或者在我们的示例中，\n",
    "# 指的是度量 ground-truth 分布与预测值之间的“距离”。\n",
    "\n",
    "# 现在，配置模型来使用优化器和损失函数：\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NpcXY9--llS"
   },
   "outputs": [],
   "source": [
    "# 创建一个验证集\n",
    "train_size = 20\n",
    "x_val = train_data[:train_size]\n",
    "partial_x_train = train_data[train_size:]\n",
    "\n",
    "y_val = train_label[:train_size]\n",
    "partial_y_train = train_label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, numpy.int64, numpy.ndarray, numpy.int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(partial_x_train), type(partial_x_train[0]), type(partial_x_train[0][0]),type(partial_y_train),type(partial_y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 487, 467, 467, 20, 20)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_label),len(partial_x_train),len(partial_y_train),len(x_val),len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6G9oqEV-Se-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 467 samples, validate on 20 samples\n",
      "Epoch 1/40\n",
      "467/467 [==============================] - 1s 1ms/sample - loss: 0.6924 - accuracy: 0.5632 - val_loss: 0.7031 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "467/467 [==============================] - 0s 192us/sample - loss: 0.6919 - accuracy: 0.5632 - val_loss: 0.7066 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "467/467 [==============================] - 0s 194us/sample - loss: 0.6915 - accuracy: 0.5632 - val_loss: 0.7101 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/40\n",
      "467/467 [==============================] - 0s 218us/sample - loss: 0.6911 - accuracy: 0.5632 - val_loss: 0.7137 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/40\n",
      "467/467 [==============================] - 0s 219us/sample - loss: 0.6908 - accuracy: 0.5632 - val_loss: 0.7172 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/40\n",
      "467/467 [==============================] - 0s 188us/sample - loss: 0.6904 - accuracy: 0.5632 - val_loss: 0.7208 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/40\n",
      "467/467 [==============================] - 0s 213us/sample - loss: 0.6900 - accuracy: 0.5632 - val_loss: 0.7244 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/40\n",
      "467/467 [==============================] - 0s 201us/sample - loss: 0.6897 - accuracy: 0.5632 - val_loss: 0.7280 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/40\n",
      "467/467 [==============================] - 0s 209us/sample - loss: 0.6894 - accuracy: 0.5632 - val_loss: 0.7316 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/40\n",
      "467/467 [==============================] - 0s 193us/sample - loss: 0.6890 - accuracy: 0.5632 - val_loss: 0.7352 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/40\n",
      "467/467 [==============================] - 0s 214us/sample - loss: 0.6887 - accuracy: 0.5632 - val_loss: 0.7388 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/40\n",
      "467/467 [==============================] - 0s 208us/sample - loss: 0.6884 - accuracy: 0.5632 - val_loss: 0.7424 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/40\n",
      "467/467 [==============================] - 0s 201us/sample - loss: 0.6882 - accuracy: 0.5632 - val_loss: 0.7460 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/40\n",
      "467/467 [==============================] - 0s 196us/sample - loss: 0.6879 - accuracy: 0.5632 - val_loss: 0.7497 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/40\n",
      "467/467 [==============================] - 0s 200us/sample - loss: 0.6876 - accuracy: 0.5632 - val_loss: 0.7533 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/40\n",
      "467/467 [==============================] - 0s 214us/sample - loss: 0.6874 - accuracy: 0.5632 - val_loss: 0.7568 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/40\n",
      "467/467 [==============================] - 0s 186us/sample - loss: 0.6872 - accuracy: 0.5632 - val_loss: 0.7604 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/40\n",
      "467/467 [==============================] - 0s 188us/sample - loss: 0.6869 - accuracy: 0.5632 - val_loss: 0.7640 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/40\n",
      "467/467 [==============================] - 0s 195us/sample - loss: 0.6867 - accuracy: 0.5632 - val_loss: 0.7675 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/40\n",
      "467/467 [==============================] - 0s 192us/sample - loss: 0.6865 - accuracy: 0.5632 - val_loss: 0.7711 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/40\n",
      "467/467 [==============================] - 0s 229us/sample - loss: 0.6864 - accuracy: 0.5632 - val_loss: 0.7745 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/40\n",
      "467/467 [==============================] - 0s 257us/sample - loss: 0.6862 - accuracy: 0.5632 - val_loss: 0.7780 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/40\n",
      "467/467 [==============================] - 0s 270us/sample - loss: 0.6860 - accuracy: 0.5632 - val_loss: 0.7814 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/40\n",
      "467/467 [==============================] - 0s 236us/sample - loss: 0.6859 - accuracy: 0.5632 - val_loss: 0.7848 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/40\n",
      "467/467 [==============================] - 0s 187us/sample - loss: 0.6858 - accuracy: 0.5632 - val_loss: 0.7881 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/40\n",
      "467/467 [==============================] - 0s 207us/sample - loss: 0.6856 - accuracy: 0.5632 - val_loss: 0.7914 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/40\n",
      "467/467 [==============================] - 0s 205us/sample - loss: 0.6855 - accuracy: 0.5632 - val_loss: 0.7946 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/40\n",
      "467/467 [==============================] - 0s 192us/sample - loss: 0.6854 - accuracy: 0.5632 - val_loss: 0.7977 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/40\n",
      "467/467 [==============================] - 0s 184us/sample - loss: 0.6853 - accuracy: 0.5632 - val_loss: 0.8008 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/40\n",
      "467/467 [==============================] - 0s 183us/sample - loss: 0.6853 - accuracy: 0.5632 - val_loss: 0.8037 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/40\n",
      "467/467 [==============================] - 0s 198us/sample - loss: 0.6852 - accuracy: 0.5632 - val_loss: 0.8065 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/40\n",
      "467/467 [==============================] - 0s 194us/sample - loss: 0.6851 - accuracy: 0.5632 - val_loss: 0.8092 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/40\n",
      "467/467 [==============================] - 0s 205us/sample - loss: 0.6851 - accuracy: 0.5632 - val_loss: 0.8118 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/40\n",
      "467/467 [==============================] - 0s 212us/sample - loss: 0.6850 - accuracy: 0.5632 - val_loss: 0.8143 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/40\n",
      "467/467 [==============================] - 0s 250us/sample - loss: 0.6850 - accuracy: 0.5632 - val_loss: 0.8167 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/40\n",
      "467/467 [==============================] - 0s 256us/sample - loss: 0.6850 - accuracy: 0.5632 - val_loss: 0.8189 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/40\n",
      "467/467 [==============================] - 0s 233us/sample - loss: 0.6849 - accuracy: 0.5632 - val_loss: 0.8211 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/40\n",
      "467/467 [==============================] - 0s 217us/sample - loss: 0.6849 - accuracy: 0.5632 - val_loss: 0.8232 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/40\n",
      "467/467 [==============================] - 0s 217us/sample - loss: 0.6849 - accuracy: 0.5632 - val_loss: 0.8251 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/40\n",
      "467/467 [==============================] - 0s 222us/sample - loss: 0.6849 - accuracy: 0.5632 - val_loss: 0.8269 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# 训练模型¶\n",
    "# 以 512 个样本的 mini-batch 大小迭代 40 个 epoch 来训练模型。这是指对 x_train 和 y_train 张量中所有样本的的 40 次迭代。\n",
    "# 在训练过程中，监测来自验证集的 10,000 个样本上的损失值（loss）和准确率（accuracy）：\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## 评估模型\n",
    "\n",
    "我们来看一下模型的性能如何。将返回两个值。损失值（loss）（一个表示误差的数字，值越低越好）与准确率（accuracy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487/1 - 0s - loss: 0.6328 - accuracy: 0.5400\n",
      "[0.6907112446653769, 0.5400411]\n"
     ]
    }
   ],
   "source": [
    "# results = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "# print(results)\n",
    "results = model.evaluate(train_data,  train_label, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcvSXvhp-llb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_accuracy', 'loss', 'val_loss', 'accuracy'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit() 返回一个 History 对象，该对象包含一个字典，其中包含训练阶段所发生的一切事件\n",
    "# 有四个条目：在训练和验证期间，每个条目对应一个监控指标。我们可以使用这些条目来绘制训练与验证过程的损失值（loss）和准确率（accuracy），以便进行比较。\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGoYf2Js-lle"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建一个准确率（accuracy）和损失值（loss）随时间变化的图表\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# “bo”代表 \"蓝点\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b代表“蓝色实线”\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hXx-xOv-llh"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 清除数字\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFEmZ5zq-llk"
   },
   "source": [
    "\n",
    "在该图中，点代表训练损失值（loss）与准确率（accuracy），实线代表验证损失值（loss）与准确率（accuracy）。\n",
    "\n",
    "注意训练损失值随每一个 epoch *下降*而训练准确率（accuracy）随每一个 epoch *上升*。这在使用梯度下降优化时是可预期的——理应在每次迭代中最小化期望值。\n",
    "\n",
    "验证过程的损失值（loss）与准确率（accuracy）的情况却并非如此——它们似乎在 20 个 epoch 后达到峰值。这是过拟合的一个实例：模型在训练数据上的表现比在以前从未见过的数据上的表现要更好。在此之后，模型过度优化并学习*特定*于训练数据的表示，而不能够*泛化*到测试数据。\n",
    "\n",
    "对于这种特殊情况，我们可以通过在 20 个左右的 epoch 后停止训练来避免过拟合。稍后，您将看到如何通过回调自动执行此操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
