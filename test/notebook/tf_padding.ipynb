{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import jieba\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = '../../data/pos/'\n",
    "neg_path = '../../data/neg/'\n",
    "pos_list = os.listdir(pos_path)\n",
    "neg_list = os.listdir(neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_list[0]),print(neg_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词词典\n",
    "stop_words = []\n",
    "# 读取停用词函数\n",
    "with open(\"../data/stop_words_line.txt\", encoding=\"utf-8\") as st:\n",
    "    stop_words = st.readline().split(\" \")\n",
    "\n",
    "def delete_stop_words(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def str_strim(line):\n",
    "    new_line = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", line)\n",
    "    return new_line.strip().strip().replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "def segment(line):\n",
    "    return delete_stop_words(jieba.cut(str_strim(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 词语索引\n",
    "index = 1\n",
    "# 所有词的索引词典\n",
    "words_dict = {}\n",
    "\n",
    "origin_data = []\n",
    "train_label = []\n",
    "for doc in pos_list:\n",
    "    with open(pos_path + doc, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = segment(line)\n",
    "            origin_data.append(words)\n",
    "            train_label.append(1)\n",
    "            for word in words:\n",
    "                if word not in words_dict:\n",
    "                    words_dict[word] = index\n",
    "                    index += 1\n",
    "            \n",
    "for doc in neg_list:\n",
    "    with open(neg_path + doc, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            words = segment(line)\n",
    "            origin_data.append(words)\n",
    "            train_label.append(0)\n",
    "            for word in words:\n",
    "                if word not in words_dict:\n",
    "                    words_dict[word] = index\n",
    "                    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_size = 20000\n",
    "# vocabulary = read_data()\n",
    "# def build_dataset(words, n_words):\n",
    "#     \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "#     count = [['UNK', -1]]\n",
    "#     count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "#     dictionary = {}\n",
    "#     for word, _ in count:\n",
    "#       dictionary[word] = len(dictionary)\n",
    "#     data = []\n",
    "#     unk_count = 0\n",
    "#     for word in words:\n",
    "#       index = dictionary.get(word, 0)\n",
    "#       if index == 0:  # dictionary['UNK']\n",
    "#         unk_count += 1\n",
    "#       data.append(index)\n",
    "#     count[0][1] = unk_count\n",
    "#     reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "#     return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "# data, count, unused_dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "\n",
    "# print('Most common words (+UNK)', count[:5])\n",
    "# print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, words_dict.popitem(), word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(origin_data), len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讲原始数据中文转化为索引\n",
    "# 将数组转换为表示单词出现与否的由 0 和 1 组成的向量，类似于 one-hot 编码。例如，序列[3, 5]将转换为一个 10,000 维的向量，该向量除了索引为 3 和 5 的位置是 1 以外，其他都为 0。然后，将其作为网络的首层——一个可以处理浮点型向量数据的稠密层。不过，这种方法需要大量的内存，需要一个大小为 num_words * num_reviews 的矩阵。\n",
    "# 或者，我们可以填充数组来保证输入数据具有相同的长度，然后创建一个大小为 max_length * num_reviews 的整型张量。我们可以使用能够处理此形状数据的嵌入层作为网络中的第一层。\n",
    "train_data_num = []\n",
    "for words in origin_data:\n",
    "    vector = []\n",
    "    for word in words:\n",
    "        if word not in words_dict:\n",
    "            continue\n",
    "        vector.append(int(words_dict.get(word)))\n",
    "    train_data_num.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_num), type(train_data_num), type(train_data_num[0]), type(train_data_num[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data = keras.preprocessing.sequence.pad_sequences([[1,2],[4,6,7,8]],padding='post',maxlen=1024)\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data_num,padding='post',maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 层按顺序堆叠以构建分类器：\n",
    "# 第一层是嵌入（Embedding）层。该层采用整数编码的词汇表，并查找每个词索引的嵌入向量（embedding vector）。这些向量是通过模型训练学习到的。向量向输出数组增加了一个维度。得到的维度为：(batch, sequence, embedding)。\n",
    "# 接下来，GlobalAveragePooling1D 将通过对序列维度求平均值来为每个样本返回一个定长输出向量。这允许模型以尽可能最简单的方式处理变长输入。\n",
    "# 该定长输出向量通过一个有 16 个隐层单元的全连接（Dense）层传输。\n",
    "# 最后一层与单个输出结点密集连接。使用 Sigmoid 激活函数，其函数值为介于 0 与 1 之间的浮点数，表示概率或置信度。\n",
    "\n",
    "# 输入形状是用于电影评论的词汇数目（10,000 词）\n",
    "vocab_size = word_count + 1\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "# 一个模型需要损失函数和优化器来进行训练。由于这是一个二分类问题且模型输出概率值（一个使用 sigmoid 激活函数的单一单元层），\n",
    "# 我们将使用 binary_crossentropy 损失函数。这不是损失函数的唯一选择，例如，您可以选择 mean_squared_error 。\n",
    "# 但是，一般来说 binary_crossentropy 更适合处理概率——它能够度量概率分布之间的“距离”，或者在我们的示例中，\n",
    "# 指的是度量 ground-truth 分布与预测值之间的“距离”。\n",
    "\n",
    "# 现在，配置模型来使用优化器和损失函数：\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.astype(np.int64)\n",
    "train_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NpcXY9--llS"
   },
   "outputs": [],
   "source": [
    "# 创建一个验证集\n",
    "train_size = 20\n",
    "x_val = train_data[:train_size]\n",
    "partial_x_train = train_data[train_size:]\n",
    "\n",
    "y_val = train_label[:train_size]\n",
    "partial_y_train = train_label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(partial_x_train), type(partial_x_train[0]), type(partial_x_train[0][0]),type(partial_y_train),type(partial_y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(train_label),len(partial_x_train),len(partial_y_train),len(x_val),len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6G9oqEV-Se-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 训练模型¶\n",
    "# 以 512 个样本的 mini-batch 大小迭代 40 个 epoch 来训练模型。这是指对 x_train 和 y_train 张量中所有样本的的 40 次迭代。\n",
    "# 在训练过程中，监测来自验证集的 10,000 个样本上的损失值（loss）和准确率（accuracy）：\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "# 模型性能 将返回两个值。损失值（loss）（一个表示误差的数字，值越低越好）与准确率（accuracy）。\n",
    "results = model.evaluate(train_data,  train_label, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcvSXvhp-llb"
   },
   "outputs": [],
   "source": [
    "# model.fit() 返回一个 History 对象，该对象包含一个字典，其中包含训练阶段所发生的一切事件\n",
    "# 有四个条目：在训练和验证期间，每个条目对应一个监控指标。我们可以使用这些条目来绘制训练与验证过程的损失值（loss）和准确率（accuracy），以便进行比较。\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGoYf2Js-lle"
   },
   "outputs": [],
   "source": [
    "# 创建一个准确率（accuracy）和损失值（loss）随时间变化的图表\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# “bo”代表 \"蓝点\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b代表“蓝色实线”\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hXx-xOv-llh"
   },
   "outputs": [],
   "source": [
    "# 在该图中，点代表训练损失值（loss）与准确率（accuracy），实线代表验证损失值（loss）与准确率（accuracy）。\n",
    "\n",
    "# 注意训练损失值随每一个 epoch 下降而训练准确率（accuracy）随每一个 epoch 上升。这在使用梯度下降优化时是可预期的——理应在每次迭代中最小化期望值。\n",
    "\n",
    "# 验证过程的损失值（loss）与准确率（accuracy）的情况却并非如此——它们似乎在 20 个 epoch 后达到峰值。这是过拟合的一个实例：模型在训练数据上的表现比在以前从未见过的数据上的表现要更好。在此之后，模型过度优化并学习特定于训练数据的表示，而不能够泛化到测试数据。\n",
    "\n",
    "# 对于这种特殊情况，我们可以通过在 20 个左右的 epoch 后停止训练来避免过拟合。稍后，您将看到如何通过回调自动执行此操作。\n",
    "plt.clf()   # 清除数字\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
